# LLM â€” æœ¬åœ°ç§æœ‰åŒ–éƒ¨ç½² + OpenAI Compatible æ¥å£ + å¤šå®¢æˆ·ç«¯ç¤ºä¾‹ï¼ˆGoï¼‰

> ç›®æ ‡ï¼šåœ¨æœ¬åœ°ç§æœ‰åŒ–éƒ¨ç½²å¤§æ¨¡å‹ï¼Œå¼€æ”¾ **OpenAI Compatible æ ‡å‡†åè®®** çš„æœåŠ¡æ¥å£ï¼Œå¹¶ç”¨ **LangChain-Go**ã€**go-openai**ã€**openai-python** ç­‰å¤šç§å®¢æˆ·ç«¯å‘èµ·è¯·æ±‚ï¼ˆé™„ Postman / cURL ç¤ºä¾‹ï¼‰ã€‚

---

## âœ¨ åŠŸèƒ½æ¦‚è§ˆ

- âœ… ä¸¤ç§æœ¬åœ°éƒ¨ç½²æ–¹å¼ï¼š
  - **Ollama**ï¼ˆä¾¿æ·å¼ä¸€é”®æ‹‰æ¨¡å‹ï¼‰
  - **llama.cpp**ï¼ˆ`llama-server` æä¾› **OpenAI å…¼å®¹**çš„ `/v1/chat/completions`ï¼‰
- âœ… æä¾› **OpenAI Compatible** çš„ HTTP APIï¼Œæ–¹ä¾¿ç›´æ¥å¤ç”¨å„ç±» OpenAI å®¢æˆ·ç«¯/SDKã€‚
- âœ… æ¼”ç¤ºå„ç±»å®¢æˆ·ç«¯è°ƒç”¨ï¼š
  - Postman
  - cURL
  - LangChain-Go æ¡†æ¶
  - go-openai æ¡†æ¶
  - openai python æ¡†æ¶
  - AutoGen python æ¡†æ¶
  - OpenAI Agent SDK Python æ¡†æ¶

---

## ğŸ“¦ ä»“åº“ç»“æ„

```bash
llm/
â”œâ”€â”€ LangChainGo/      # LangChain-Go
â”‚ â””â”€â”€ main.go
â”œâ”€â”€ OpenAIGo/         # OpenAI-Go
â”‚ â””â”€â”€ main.go
â”œâ”€â”€ OpenAIPython/     # OpenAI Python
â”‚ â””â”€â”€ main.py
â”œâ”€â”€ AutoGen/          # AutoGen Python
â”‚ â””â”€â”€ main.py
â””â”€â”€ OpenAI-Agent-SDK/  # OpenAI Agent SDK Python
  â””â”€â”€ main.py
```

---

## ğŸš€ ç§æœ‰åŒ–éƒ¨ç½²

### æ–¹æ¡ˆä¸€ï¼šOllamaï¼ˆä¸Šæ‰‹æœ€ç®€å•ï¼‰

1. å®‰è£… [Ollama](https://ollama.com/download)ï¼Œå¯é€‰æ‹©ä¸åŒå¹³å°ã€‚
2. è¿›è¡Œæ¨¡å‹çš„é€‰æ‹©ï¼Œä¾‹å¦‚ `deepseek-r1:8b`ï¼Œç„¶ååœ¨å‘½ä»¤è¡Œè¾“å…¥ï¼š
   ```
   ollama run deepseek-r1:8b
   ```
   å³å¯ä¸‹è½½æ¨¡å‹å¹¶å¯åŠ¨å¯¹è¯
3. äº¤äº’å¼å¯¹è¯å¯åŠ¨çš„åŒæ—¶ï¼ŒOllama ä¹Ÿæä¾›è®¿é—®æ¥å£

- POST http://localhost:11434/api/generate ï¼Œæ˜¯ **Ollama åŸç”Ÿ**ç”Ÿæˆæ¥å£ï¼ˆ**ä¸æ˜¯** OpenAI è§„èŒƒï¼‰
- POST http://localhost:11434/v1/chat/completions ï¼Œæ˜¯ **OpenAI å…¼å®¹**çš„ç«¯ç‚¹ï¼Œå¯å‚è€ƒ [Ollama](https://ollama.com/blog/openai-compatibility?utm_source=chatgpt.com)

> **/api/generate å¯é€‰å‚æ•°ï¼ˆèŠ‚é€‰ï¼‰**  
> è¯·æ±‚å½¢å¦‚ï¼š
>
> ```json
> {
>   "model": "deepseek-r1:8b",
>   "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
>   "stream": true,
>   "options": {
>     "num_predict": 512,
>     "temperature": 0.8,
>     "top_p": 0.95,
>     "top_k": 40,
>     "min_p": 0.05,
>     "repeat_penalty": 1.1,
>     "repeat_last_n": 64,
>     "presence_penalty": 0,
>     "frequency_penalty": 0,
>     "seed": 42,
>     "stop": ["</think>"],
>     "num_ctx": 8192,
>     "mirostat": 0,
>     "mirostat_tau": 5.0,
>     "mirostat_eta": 0.1
>   },
>   "format": "json",
>   "keep_alive": "5m"
> }
> ```
>
> _è¯´æ˜_ï¼š`options` å†…çš„å¤§å¤šæ•°é‡‡æ ·ä¸æƒ©ç½šå‚æ•°ä¸ç¤¾åŒºå¸¸è§å®ç°ä¸€è‡´ï¼›å®é™…æ”¯æŒé¡¹ä»¥ Ollama å®˜æ–¹æ–‡æ¡£ä¸ºå‡†ã€‚ ([Postman](https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api?utm_source=chatgpt.com), [Medium](https://medium.com/%40laurentkubaski/ollama-chat-endpoint-parameters-21a7ac1252e5?utm_source=chatgpt.com))

---

### æ–¹æ¡ˆäºŒï¼šllama.cppï¼ˆOpenAI å…¼å®¹é¦–é€‰ï¼‰

1. å®‰è£…æ–¹æ³•è¯¦è§ [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md)ï¼Œè‹¥æ˜¯ windows æ“ä½œç³»ç»Ÿåˆ™å¯ä»¥ä½¿ç”¨ Winget åœ¨å‘½ä»¤è¡Œå®‰è£…ï¼š

   ```bash
   winget install llama.cpp
   ```

2. ä» [Hugging Face](https://huggingface.co) ä¸‹è½½åˆé€‚ä½ æœºå™¨çš„ GGUF æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š`unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF`

3. å¯åŠ¨ HTTP æœåŠ¡ï¼ˆOpenAI å…¼å®¹ï¼‰ï¼š
   ```bash
   llama-server -m /path/to/model.gguf --port 8080
   # è®¿é—® UI: http://localhost:8080
   # Chat ç«¯ç‚¹: http://localhost:8080/v1/chat/completions
   ```
   `llama-server` è‡ªå¸¦ **OpenAI API å…¼å®¹**çš„ `/v1/chat/completions`ï¼Œå¤§å¤šæ•° OpenAI å®¢æˆ·ç«¯å¯ç›´æ¥å¤ç”¨ï¼ˆé€šè¿‡è‡ªå®šä¹‰ base_urlï¼‰ã€‚

---

## ğŸ”Œ å¦‚ä½•å‘èµ·è¯·æ±‚

ä»¥ä¸‹ç¤ºä¾‹åŸºäº OpenAI å…¼å®¹ç«¯ç‚¹
**ollama**: `http://localhost:11434/v1/chat/completions`
**llama.cpp** ï¼š`http://localhost:8080/v1/chat/completions`

### 1) Postman

#### æµå¼è¾“å‡º

- **POST** `http://localhost:8080/v1/chat/completions`
- Headersï¼š`Content-Type: application/json`
- Bodyï¼ˆraw JSONï¼‰ï¼š

  ```json
  {
    "model": "deepseek-r1:8b",
    "messages": [{ "role": "user", "content": "ä½ å¥½" }],
    "stream": true
  }
  ```

- ç»“æœå°†ä»¥ **SSE æµå¼**è¿”å›è‹¥å¹² `data:` è¡Œï¼ˆ`object=chat.completion.chunk`ï¼Œå­—æ®µä¸º `delta.content`ï¼‰ï¼Œç›´åˆ° `finish_reason`ã€‚è¿™æ˜¯ OpenAI æµå¼çº¦å®šï¼Œå¤šæ•° SDK ä¼šè‡ªåŠ¨è§£æã€‚

  ```
  data: {"choices":[{"finish_reason":null,"index":0,"delta":{"content":"ä¸ºä½ "}}],"created":1755572389,"id":"chatcmpl-vkguzwQo725GhaGbUws0zoAKA2ZC7WIZ","model":"gpt-3.5-turbo","system_fingerprint":"b6178-5e6229a8","object":"chat.completion.chunk"}

  data: {"choices":[{"finish_reason":null,"index":0,"delta":{"content":"æœåŠ¡"}}],"created":1755572389,"id":"chatcmpl-vkguzwQo725GhaGbUws0zoAKA2ZC7WIZ","model":"gpt-3.5-turbo","system_fingerprint":"b6178-5e6229a8","object":"chat.completion.chunk"}

  data: {"choices":[{"finish_reason":null,"index":0,"delta":{"content":"ã€‚"}}],"created":1755572389,"id":"chatcmpl-vkguzwQo725GhaGbUws0zoAKA2ZC7WIZ","model":"gpt-3.5-turbo","system_fingerprint":"b6178-5e6229a8","object":"chat.completion.chunk"}

  data: {"choices":[{"finish_reason":null,"index":0,"delta":{"content":"ğŸ˜Š"}}],"created":1755572389,"id":"chatcmpl-vkguzwQo725GhaGbUws0zoAKA2ZC7WIZ","model":"gpt-3.5-turbo","system_fingerprint":"b6178-5e6229a8","object":"chat.completion.chunk"}

  data: {"choices":[{"finish_reason":"stop","index":0,"delta":{}}],"created":1755572389,"id":"chatcmpl-vkguzwQo725GhaGbUws0zoAKA2ZC7WIZ","model":"gpt-3.5-turbo","system_fingerprint":"b6178-5e6229a8","object":"chat.completion.chunk","usage":{"completion_tokens":32,"prompt_tokens":4,"total_tokens":36},"timings":{"prompt_n":4,"prompt_ms":63.998,"prompt_per_token_ms":15.9995,"prompt_per_second":62.50195318603706,"predicted_n":32,"predicted_ms":914.138,"predicted_per_token_ms":28.5668125,"predicted_per_second":35.00565560123307}}

  data: [DONE]
  ```

> å…³äºä½ åœ¨æµé‡Œçœ‹åˆ°çš„ `<think> ... </think>`ï¼šè¿™æ˜¯è®¸å¤šâ€œæ¨ç†/æ€è€ƒå¼â€æ¨¡å‹å¸¸è§çš„å¯è§æ ‡è®°ï¼ˆä¾¿äºä¸å¯è§å›ç­”åŒºåˆ†ï¼‰ã€‚å¦‚ä¸éœ€è¦ï¼Œå¯é€šè¿‡ **æç¤º**æˆ– **åœæ­¢è¯**ï¼ˆä¾‹å¦‚ `"</think>"`ï¼‰æ¥æŠ‘åˆ¶æˆ–æˆªæ–­åœ¨æœ€ç»ˆè¾“å‡ºä¸­çš„å‡ºç°ï¼ˆæ˜¯å¦å®Œå…¨å¯æ§å–å†³äºå…·ä½“æ¨¡å‹/æœåŠ¡å‚æ•°ï¼‰ã€‚

- æµå¼è¾“å‡ºæ•ˆæœå¦‚ä¸‹ï¼š
  ![Postmanæµå¼è¾“å‡º](./assets/Postmanæµå¼è¾“å‡º.png)

#### éæµå¼è¾“å‡º

- **POST** `http://localhost:8080/v1/chat/completions`
- Headersï¼š`Content-Type: application/json`
- Bodyï¼ˆraw JSONï¼‰ï¼š

  ```json
  {
    "model": "deepseek-r1:8b",
    "messages": [{ "role": "user", "content": "ä½ å¥½" }],
    "stream": false // æ­¤å¤„æ”¹ä¸ºéæµå¼è¾“å‡º
  }
  ```

- ç»“æœå°†ç›´æ¥å•æ¡è¿”å›
  ```json
  {
    "choices": [
      {
        "finish_reason": "stop",
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "<think>\n\n</think>\n\nä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿ"
        }
      }
    ],
    "created": 1755593019,
    "model": "gpt-3.5-turbo",
    "system_fingerprint": "b6178-5e6229a8",
    "object": "chat.completion",
    "usage": {
      "completion_tokens": 17,
      "prompt_tokens": 4,
      "total_tokens": 21
    },
    "id": "chatcmpl-DmuTYEekHwbauyJn5jUWZiVTYoeiFcRo",
    "timings": {
      "prompt_n": 1,
      "prompt_ms": 41.528,
      "prompt_per_token_ms": 41.528,
      "prompt_per_second": 24.080138701598923,
      "predicted_n": 17,
      "predicted_ms": 426.314,
      "predicted_per_token_ms": 25.07729411764706,
      "predicted_per_second": 39.87671059360002
    }
  }
  ```
- éæµå¼è¾“å‡ºæ•ˆæœå¦‚ä¸‹ï¼š
  ![Postmanéæµå¼è¾“å‡º](./assets/Postmanéæµå¼è¾“å‡º.png)

### 2) cURL

**éæµå¼ï¼š**

```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-deepseek-r1",
    "messages": [{"role":"user","content":"ä½ å¥½"}],
    "stream": false,
    "max_tokens": 256,
    "temperature": 0.7
  }'
```

**æµå¼ï¼š**

```bash
curl -N http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -d '{
    "model": "local-deepseek-r1",
    "messages": [{"role":"user","content":"ä½ å¥½"}],
    "stream": true
  }'
```

> è¯´æ˜ï¼š`-N` å…³é—­ç¼“å†²ï¼Œå®æ—¶æ‰“å°æ¯æ¡`data:`ã€‚ ([OpenAI å¹³å°](https://platform.openai.com/docs/api-reference?utm_source=chatgpt.com))

---

### 3) LangChain-Go

```go
package main

import (
	"context"
	"log"

	"github.com/tmc/langchaingo/llms"
	"github.com/tmc/langchaingo/llms/openai"
)

func main() {
	log.SetFlags(log.LstdFlags | log.Lshortfile)

	ctx := context.Background()

	llm, err := openai.New(
		openai.WithBaseURL("http://localhost:8080/v1"), // æŒ‡å‘ OpenAI å…¼å®¹ç«¯ç‚¹
		openai.WithToken("xxxxx"),                      // æœ¬åœ°æœåŠ¡é€šå¸¸ä¸ä¼šæ ¡éªŒï¼Œå¯å¡«å ä½
	)
	if err != nil {
		log.Fatal(err)
	}

	prompt := "ä½ å¥½ï¼Œè¯·é—®ä½ èƒ½åšäº›ä»€ä¹ˆå‘¢?"

	msgs := []llms.MessageContent{
		{Role: llms.ChatMessageTypeHuman, Parts: []llms.ContentPart{llms.TextPart(prompt)}},
	}

	res, err := llm.GenerateContent(ctx, msgs)
	if err != nil {
		log.Fatal(err)
	}
	for _, c := range res.Choices {
		log.Println(c.Content)
	}
}
```

### 4) go-openaiï¼ˆsashabaranov/go-openaiï¼‰

> å…³é”®åœ¨äº**è‡ªå®šä¹‰ BaseURL** æŒ‡å‘ä½ çš„æœ¬åœ° OpenAI å…¼å®¹ç«¯ç‚¹ï¼›API Key å¯ç”¨å ä½å€¼ã€‚ ([Go Packages](https://pkg.go.dev/github.com/sashabaranov/go-openai?utm_source=chatgpt.com), [GitHub](https://github.com/sashabaranov/go-openai?utm_source=chatgpt.com))

```go
package main

import (
	"context"
	"encoding/json"
	"errors"
	"io"
	"log"

	openai "github.com/sashabaranov/go-openai"
)

func main() {
	log.SetFlags(log.LstdFlags | log.Lshortfile)

	// è‡ªå®šä¹‰é…ç½®ï¼šæŒ‡å‘æœ¬åœ°æœåŠ¡å™¨
	client := openai.NewClientWithConfig(
		openai.DefaultAnthropicConfig("xx", "http://localhost:8080/v1"),
	)

	prompt := "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹è‡ªå·±"

	// === æµå¼ ===
	stream, err := client.CreateChatCompletionStream(
		context.Background(),
		openai.ChatCompletionRequest{
			Model: openai.GPT3Dot5Turbo, // è®¸å¤šå…¼å®¹æœåŠ¡ä¼šå¿½ç•¥è¿™é‡Œçš„å€¼ï¼Œè§ä¸‹æ–¹ FAQ
			Messages: []openai.ChatCompletionMessage{
				{Role: openai.ChatMessageRoleUser, Content: prompt},
			},
			Stream: true,
		},
	)
	if err != nil {
		log.Fatalf("ChatCompletion error: %v\n", err)
	}
	defer stream.Close()

	log.Println("Stream response:")
	for {
		resp, err := stream.Recv()
		if errors.Is(err, io.EOF) {
			log.Println("Stream finished")
			break
		}
		if err != nil {
			log.Fatalf("Stream error: %v\n", err)
		}
		b, _ := json.Marshal(resp)
		log.Println(string(b))
	}

	// === éæµå¼ï¼ˆå¦‚éœ€ï¼‰===
	// resp, err := client.CreateChatCompletion(context.Background(), openai.ChatCompletionRequest{
	//   Model: openai.GPT3Dot5Turbo,
	//   Messages: []openai.ChatCompletionMessage{{Role: openai.ChatMessageRoleUser, Content: prompt}},
	//   Stream: false,
	// })
	// if err != nil { log.Fatal(err) }
	// log.Println(resp.Choices[0].Message.Content)
}
```

### 5) openai python

```python
from openai import OpenAI

client = OpenAI(
   base_url = 'http://localhost:8080/v1',
   api_key='xx', # required, but unused
)

allMessages = [
   {"role": "system", "content": "You are a helpful assistant."},
   {"role": "user", "content": "Who won the world series in 2020?"},
   {"role": "assistant", "content": "The LA Dodgers won in 2020."},
   {"role": "user", "content": "Where was it played?"}
 ]

# === æµå¼è¾“å‡º ===
stream = client.chat.completions.create(
 model="deepseek-r1:1.5b",
 messages=allMessages,
 stream=True
)
for event in stream:
   print(event)


# # === éæµå¼è¾“å‡º ===
# response = client.chat.completions.create(
#   model="deepseek-r1:1.5b",
#   messages=allMessages
# )
# print(response.choices[0].message.content)
```

### 6) AutoGen python

```python
from autogen import AssistantAgent, UserProxyAgent

config_list = [
  {
    "model": "deepseek-r1:8b",
    "base_url": "http://localhost:8080/v1",
    "api_key": "xxx",
  }
]

assistant = AssistantAgent("assistant", llm_config={"config_list": config_list})

user_proxy = UserProxyAgent(
    "user_proxy",
    code_execution_config={"work_dir": "coding", "use_docker": False},
    human_input_mode="NEVER",                       # å…³é”®å‚æ•°ï¼šç¦ç”¨äººå·¥è¾“å…¥
    max_consecutive_auto_reply=5,                   # æœ€å¤§è‡ªåŠ¨å›å¤æ¬¡æ•°
    default_auto_reply="ä½ åˆ†æçš„å¯¹ï¼Œè¯·ç»§ç»­åˆ†æ"       # è‡ªåŠ¨å›å¤ assistant
)
user_proxy.initiate_chat(assistant, message="ä½ å¥½è¯·ä½ ä»‹ç»ä¸€ä¸‹è‡ªå·±")  # Plot a chart of NVDA and TESLA stock price change YTD

```

### 6) AutoGen python

```python
import asyncio
from openai import AsyncOpenAI
from agents import (
    Agent,
    Runner,
    set_default_openai_api,
    set_default_openai_client,
    set_tracing_disabled,
)
from openai.types.responses import ResponseTextDeltaEvent

# =========== é…ç½®åŒº ===========
BASE_URL = "http://localhost:8080/v1"
API_KEY = "dummy_key"   # æœ¬åœ°æ¨¡å‹ä¸€èˆ¬ä¸ç”¨éªŒè¯ï¼Œéšä¾¿å†™å³å¯
MODEL_NAME = "deepseek-r1:1.5b"
isStream = True         # æµå¼è¾“å‡º
inputMessage = "ä½ å¥½"   # æé—®å†…å®¹

# =========== åˆå§‹åŒ–å…¨å±€è®¾ç½® ===========
set_default_openai_api("chat_completions")
set_default_openai_client(AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY))
# åœ¨æ­¤ç¤ºä¾‹ä¸­ç¦ç”¨è¿½è¸ª
# # å¦‚éœ€ä½¿ç”¨è‡ªå®šä¹‰è¿½è¸ªå¤„ç†å™¨ï¼Œè¯·å‚è€ƒï¼šhttps://openai.github.io/openai-agents-python/tracing/#external-tracing-processors-list
set_tracing_disabled(disabled=True)
agent = Agent(
    name="Assistant",
    instructions="You are a helpful assistant",
    model=MODEL_NAME
)

# =========== éæµå¼æ‰§è¡Œ ===========
def run_sync_mode(user_input: str):
    result = Runner.run_sync(agent, user_input)
    print("\n=== Final Output ===")
    print(result.final_output)

# =========== æµå¼æ‰§è¡Œ ===========
async def run_stream_mode(user_input: str):
    result = Runner.run_streamed(agent, input=user_input)
    print("\n=== Streaming Output ===")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)
    print()  # æ¢è¡Œ

# =========== ä¸»å…¥å£ ===========
if __name__ == "__main__":
    if isStream:
        asyncio.run(run_stream_mode(inputMessage))
    else:
        run_sync_mode(inputMessage)
```

---

## ğŸ› ï¸ è¿è¡Œä¸å¼€å‘

### 1ï¼‰å‡†å¤‡ç¯å¢ƒ

- Go 1.20+ï¼ˆå»ºè®®ï¼‰
- å·²ç»æŒ‰â€œç§æœ‰åŒ–éƒ¨ç½²â€å°èŠ‚å¯åŠ¨å¥½ **Ollama** æˆ– **llama.cpp** æœåŠ¡

### 2ï¼‰è¿è¡Œç¤ºä¾‹

```bash
# LangChain-Go
cd llm/LangChainGo
go mod tidy
go run main.go

# go-openai
cd llm/OpenAIGo
go mod tidy
go run main.go

# openai python
cd llm/OpenAIPython
pip install openai
python main.py

# AutoGen python
cd llm/AutoGen
pip install pyautogen
pip install autogen
pip install openai==1.66.2
pip install ag2[openai]
python main.py

# OpenAI Agent SDK Python
pip install openai-agents
python main.py
```

### 3ï¼‰å¸¸è§é—®é¢˜

- **401 æˆ–é‰´æƒå¤±è´¥**ï¼šå¤§å¤šä¸ºå®¢æˆ·ç«¯å¼ºåˆ¶éœ€è¦ Keyã€‚å¯åœ¨ Header é‡Œéšä¾¿æ”¾ä¸€ä¸ª `Authorization: Bearer local`ï¼Œæˆ–åœ¨ go-openai é‡Œä¸å¡«/å¡«å ä½ã€‚
- **æµå¼æ— è¾“å‡º**ï¼šcURL è®°å¾—åŠ  `-N`ï¼ŒPostman é€‰æ‹© SSE/äº‹ä»¶æµæŸ¥çœ‹å™¨ï¼›æŸäº›ä»£ç†/ç½‘å…³ä¼šç¼“å†² SSEã€‚

---

## ğŸ” å‚è€ƒèµ„æ–™ï¼ˆèŠ‚é€‰ï¼‰

- llama.cpp `llama-server`ï¼ˆOpenAI å…¼å®¹ç«¯ç‚¹ä¸å¯åŠ¨ç¤ºä¾‹ï¼‰ã€‚ ([GitHub](https://github.com/ggml-org/llama.cpp?utm_source=chatgpt.com))

- å…³äºå…¼å®¹ç«¯ç‚¹å“åº”ä¸­çš„ `model` å­—æ®µä¸å…¼å®¹æ€§è®¨è®ºï¼ˆissueï¼‰ã€‚ ([GitHub](https://github.com/ggml-org/llama.cpp/issues/4218?utm_source=chatgpt.com), [Llama CPP Python](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/?utm_source=chatgpt.com))

- Ollama APIï¼ˆ`/api/generate`ã€`/api/chat` ä¸ OpenAI å…¼å®¹è¯´æ˜ï¼‰ã€‚ ([Postman](https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api?utm_source=chatgpt.com), [Medium](https://medium.com/%40laurentkubaski/ollama-chat-endpoint-parameters-21a7ac1252e5?utm_source=chatgpt.com), [Ollama](https://ollama.com/blog/openai-compatibility?utm_source=chatgpt.com))

- go-openaiï¼ˆè‡ªå®šä¹‰ BaseURL / å…¼å®¹ç«¯ç‚¹æ¥å…¥ï¼‰ã€‚ ([Go Packages](https://pkg.go.dev/github.com/sashabaranov/go-openai?utm_source=chatgpt.com), [GitHub](https://github.com/sashabaranov/go-openai?utm_source=chatgpt.com))

- openai pythonï¼ˆè°ƒç”¨æ¥å…¥å‚è€ƒï¼‰ï¼ˆ[PyPI](https://pypi.org/project/openai/)ï¼‰

- AutoGen pythonï¼ˆè°ƒç”¨æ¥å…¥å‚è€ƒï¼‰ï¼ˆ[PyPI](https://pypi.org/project/autogen/)ï¼‰

- OpenAI API å‚è€ƒï¼ˆSSE / Chat Completions è¯­ä¹‰ï¼‰ã€‚ ([OpenAI å¹³å°](https://platform.openai.com/docs/api-reference/chat/create))

- OpenAI Agent SDK Pythonï¼ˆè°ƒç”¨æ¥å…¥å‚è€ƒï¼‰ï¼ˆ[PyPI](https://pypi.org/project/openai-agents/), [OpenAI](https://openai.github.io/openai-agents-python/quickstart/), [Github](https://github.com/openai/openai-agents-python)ï¼‰
